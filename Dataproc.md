# Dataproc

- Dataproc is fully managed service for running Hadoop and Spark data processing workloads. 
- Cluster type
  - Standard (1 master, N workers)
  - Single Node (1 master, 0 workers)
  - High Availability (3 masters, N workers)
- Worker node regular VM or Preemptible VM (Cost reduction)
- Job Supported: Hadoop, SparkR, Spark, SparkSQL, Hive, Pig, PySpark
- Monitoring
  - Job Driver Output - The best way to find what error caused a Spark job failure is to look at the driver output and the logs generated by the Spark executioners. It's possible to see the logs for each container from the spark app Web UI, or from the history server after the program ends in the executer's tab. You need to browse through each Spark container to view each log.
  - Cloud Monotoring - can monitor the cluster's CPU, disk, network usage and Yarn resources.
  - Cloud Logging - provides a consolidated and concise view of all logs so that you don't need to spend time browsing among container logs to find errors.


![image](https://user-images.githubusercontent.com/19702456/222905812-b96b7e40-6a27-4dd8-ae91-341dd2125dfc.png)

Inside of a Google data center, the internal name for the massively distributed storage layer is called Colossus. Under the network inside the data center is Jupyter.

![image](https://user-images.githubusercontent.com/19702456/222905822-8241a5b3-9eb7-4c9e-89b2-093384a41e10.png)

In general, you want to use a push-based model for any data that you know you will need while pull-based may be a useful model if there is a lot of data that you might not ever need to migrate.
To get the most from Dataproc, customers need to move to an ephemeral model of only using clusters when they need them.

- To locate the default Cloud Storage bucket used by Dataproc: `gcloud dataproc clusters describe sparktodp --region=us-central1 --format=json | jq -r '.config.configBucket'`



<img width="440" alt="image" src="https://github.com/user-attachments/assets/72750345-3462-45dc-802c-24076c9aedb8">

Optimizing Dataproc:
- Make sure that the Cloud storage bucket is in the same regional location as your Dataproc region.
- Be sure that you do not have any network rules or roots that funnel Cloud storage traffic through a small number of VPN gateways before it reaches your cluster.
- Make sure you are not dealing with more than around 10,000 input files. If you find yourself in this situation try to combine or union the data into larger file sizes.
- If this file volume reduction means that now you are working with larger datasets more than approximately 50,000 Hadoop partitions you should consider adjusting the setting fs.gs.block.size to a larger value accordingly.
- Is the size of your persistent disk limiting your throughput? Oftentimes when getting started with Google Cloud you may have just a small table that you want to benchmark. This is generally a good approach as long as you do not choose a persistent disk that assigns to such a small quantity of data, it will most likely limit your performance. Standard persistent disk scale linearly with volume size.
- Did you allocate enough virtual machines to your cluster? Running prototypes and benchmarking with real data and real jobs is crucial to informing the actual VM allocation decision. Employing job scoped clusters is a common strategy for Dataproc clusters
- Local HDFS is a good option if:
  - Your jobs require a lot of metadata operationsâ€”for example, you have thousands of partitions and directories, and each file size is relatively small.
  - You modify the HDFS data frequently or you rename directories. Cloud Storage objects are immutable, so renaming a directory is an expensive operation because it consists of copying all objects to a new key and deleting them afterwards.
  - You heavily use the append operation on HDFS files.
  - You have workloads that involve heavy I/O. For example, you have a lot of partitioned writes, such as in this example `spark.read.write.partitionBy(...).parquet("gs://")`
  - You have I/O workloads that are especially sensitive to latency. For example, you require single-digit millisecond latency per storage operation.





